# Hybrid ConvNeXtV2-Swin Transformer Configuration
# MENDICANT_BIAS Phase 3 - Advanced Architecture
# Agent: HOLLOWED_EYES

# Model Architecture
model:
  architecture: "hybrid_convnextv2_swin"

  # ConvNeXtV2 Configuration
  convnextv2:
    variant: "base"  # Options: tiny, small, base, large
    depths: [3, 3, 27]  # Blocks per stage (stage 4 replaced by Swin)
    dims: [128, 256, 512]  # Channels per stage
    drop_path_rate: 0.1
    layer_scale_init_value: 1.0e-6
    use_grn: true  # Global Response Normalization
    pretrained: false

  # Swin Transformer Configuration
  swin:
    variant: "small"  # Options: tiny, small, base
    depths: [2, 6]  # 2 stages for global context
    num_heads: [4, 8]
    embed_dim: 128
    window_size: 7
    mlp_ratio: 4.0
    qkv_bias: true
    drop_path_rate: 0.2
    pretrained: false

  # Multi-Scale Fusion Module
  fusion:
    method: "pyramid"  # pyramid, concat, attention
    fusion_dim: 768
    use_attention: true

  # Classification Head
  head:
    num_classes: 7  # HAM10000 diagnoses
    dropout: 0.3
    use_batch_norm: true

  # FairDisCo Integration (Optional)
  fairdisco:
    enable: false  # Set true to enable adversarial debiasing
    num_fst_classes: 6  # FST I-VI
    lambda_adv: 0.3
    lambda_con: 0.2


# Training Configuration
training:
  # Basic Settings
  epochs: 100
  batch_size: 32  # Adjust based on GPU memory
  accumulation_steps: 2  # Effective batch size = 64
  num_workers: 4
  pin_memory: true

  # Learning Rate
  learning_rate: 0.0001
  weight_decay: 0.05

  # Scheduling
  scheduler:
    type: "cosine"  # cosine, step, exponential
    t_max: 100
    eta_min: 0.000001
    warmup_epochs: 10
    warmup_start_lr: 0.00001

  # Regularization
  label_smoothing: 0.1
  mixup_alpha: 0.0  # Set > 0 to enable mixup
  cutmix_alpha: 0.0  # Set > 0 to enable cutmix

  # Mixed Precision Training
  use_amp: true  # Automatic Mixed Precision
  grad_clip: 1.0  # Gradient clipping

  # Multi-Stage Training (Optional)
  freeze_convnext_epochs: 0  # Freeze ConvNeXt for first N epochs
  freeze_swin_epochs: 0  # Freeze Swin for first N epochs


# Optimizer Configuration
optimizer:
  type: "adamw"
  betas: [0.9, 0.999]
  eps: 1.0e-8


# Data Configuration
data:
  dataset: "ham10000"
  data_dir: "data/HAM10000"
  image_size: 224

  # Data Augmentation
  augmentation:
    train:
      - type: "resize"
        size: 256
      - type: "random_crop"
        size: 224
      - type: "horizontal_flip"
        p: 0.5
      - type: "vertical_flip"
        p: 0.5
      - type: "rotation"
        degrees: 20
      - type: "color_jitter"
        brightness: 0.2
        contrast: 0.2
        saturation: 0.2
        hue: 0.1
      - type: "normalize"
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

    val:
      - type: "resize"
        size: 256
      - type: "center_crop"
        size: 224
      - type: "normalize"
        mean: [0.485, 0.456, 0.406]
        std: [0.229, 0.224, 0.225]

  # Class Balancing
  use_weighted_sampler: true
  oversample_minority: false


# Checkpointing
checkpointing:
  save_dir: "checkpoints/hybrid"
  save_frequency: 5  # Save every N epochs
  save_best_only: false
  monitor: "val_auroc"  # Metric to monitor for best model
  mode: "max"


# Logging
logging:
  log_dir: "logs/hybrid"
  use_tensorboard: true
  use_wandb: false  # Set true to enable Weights & Biases
  wandb_project: "mendicant_bias_phase3"
  wandb_entity: null
  log_interval: 10  # Log every N batches


# Evaluation
evaluation:
  # Fairness Metrics
  compute_fairness: true
  fst_groups: [1, 2, 3, 4, 5, 6]  # FST I-VI

  # Performance Metrics
  metrics:
    - "accuracy"
    - "balanced_accuracy"
    - "auroc"
    - "f1_macro"
    - "precision"
    - "recall"

  # Fairness Metrics
  fairness_metrics:
    - "auroc_gap"  # Max AUROC difference across FSTs
    - "equal_opportunity_diff"  # EOD
    - "demographic_parity_diff"  # DPD
    - "calibration_ece"  # Expected Calibration Error

  # Per-class and per-FST analysis
  generate_confusion_matrix: true
  generate_roc_curves: true
  generate_per_fst_metrics: true


# Hardware
hardware:
  device: "cuda"  # cuda or cpu
  cuda_visible_devices: "0"
  deterministic: false  # Set true for reproducibility (slower)
  benchmark: true  # cuDNN benchmark mode (faster)


# Reproducibility
seed: 42


# Expected Performance Targets
# ==============================
# Baseline ResNet50: AUROC 88.5%, AUROC Gap 0.18
# Hybrid (Phase 3): AUROC 91-93%, AUROC Gap 0.02
# Hybrid + FairDisCo: AUROC 93-95%, AUROC Gap 0.01
# Training Time: ~60-80 GPU hours
